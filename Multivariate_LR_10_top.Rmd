---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
plot(cars)
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

```{r}
# Preparing data frame for multivariate logistic regression of top 10 features from backward selection model 

rm(list=ls())

# Loading in relevant libraries 

library(caret) 
library(dplyr)
library(tidyverse)
library(broom)
library(ggplot2)
library(dplyr)


Comfort_Q18 <- read.csv("final.preprocessed_df_21.10_noSQ.csv") #loading in preprocessed data frame

Comfort_Q20<- Comfort_Q18[-c(1)] # removing x column, byproduct of loading in csv file  

# Selecting top 10 features in multivariate logistic regression model with backward selection 

Comfort_Q20<- Comfort_Q20[ , which(names(Comfort_Q20) %in% c("Charlson.Mild","Charlson.Severe","Frail","Antibiotics","Clopidogrel","Anticoagulant","Replacement", "Loop.diuretics","Statins","Aspirin","Upgrade","Outcome_variable"))]

```


```{r}
# Splitting into train and test splits 

set.seed(944935481)

trainIndex <- createDataPartition(Comfort_Q20$Outcome_variable, p = .5, 
                                  list = FALSE, 
                                  times = 1) #creating test and training partitions 
head(trainIndex)

Train <- Comfort_Q20[ trainIndex,] #obtaining training set 
Test  <- Comfort_Q20[-trainIndex,] #obtaining test set  

print(table(Test$Outcome_variable)) #checking appropriate splitting of outcome variable n=26 
print(table(Train$Outcome_variable)) #n = 28


# SMOTE implementation 

library(smotefamily)

DATA <-SMOTE(Train, Train$Outcome_variable, K=5, dup_size = 10) #implementing SMOTE. 5 nearest neighbours during sampling process. Duplicating the number of minority instances (death with no appropriate therapy) 10 times. Therefore, minority class is 11x bigger in absolute terms  

Comfort_Q12 <- DATA$data #extracting SMOTE dataset

print(prop.table(table(Comfort_Q12$Outcome_variable))) # proportions in outcome variable following SMOTE 

print(table(Comfort_Q12$Outcome_variable)) # 303(0):308(1) -> almost perfectly equal proportions in outcome variable 

Comfort_Q12 <-Comfort_Q12[ , -which(names(Comfort_Q12) %in% c("class"))] # removing class column (matches up perfectly with outcome variable column) - byproduct of SMOTE

# Reformatting binary variables to factors where appropriate  
 
Comfort_Q12$Statins<-factor(ifelse(Comfort_Q12$Statins<0.5,0,1))
Comfort_Q12$Aspirin<-factor(ifelse(Comfort_Q12$Aspirin<0.5,0,1))
Comfort_Q12$Clopidogrel<-factor(ifelse(Comfort_Q12$Clopidogrel<0.5,0,1))
Comfort_Q12$Loop.diuretics<-factor(ifelse(Comfort_Q12$Loop.diuretics<0.5,0,1))
Comfort_Q12$Anticoagulant<-factor(ifelse(Comfort_Q12$Anticoagulant<0.5,0,1))
Comfort_Q12$Replacement <- factor(ifelse(Comfort_Q12$Replacement<0.5,0,1))
Comfort_Q12$Frail <- (ifelse(Comfort_Q12$Frail<0.5,0,1))
Comfort_Q12$Charlson.Mild <- (ifelse(Comfort_Q12$Charlson.Mild<0.5,0,1))
Comfort_Q12$Charlson.Severe <- (ifelse(Comfort_Q12$Charlson.Severe<0.5,0,1))
Comfort_Q12$Upgrade <- factor(ifelse(Comfort_Q12$Upgrade<0.5,0,1))

Train <- Comfort_Q12 # saving to new variable

Train$Outcome_variable <-as.factor(Train$Outcome_variable) #converting outcome column to factor 

```

```{r}

#Multivariate logistic regression with top 15 features  

k <- 10 #number of iterations 

# Accuracy
acc <- NULL # for storing accuracy from each iteration of cross-validation

performances <- c() # # for storing RMSE values from each iteration of                                                   cross-validation 

# Fitting logistic regression model with repeated stratified cross validation

set.seed(8699297)


for(i in 1:k)
{
# splitting of train data into 90% train, 10% validation 
trainIndex <- createDataPartition(Train$Outcome_variable, p = 0.9,
                                 list = FALSE,
                                 times = 1)
train <- Train[trainIndex, ] 
validation <- Train[-trainIndex, ]

# Fitting the model 
model_x <- glm(Outcome_variable ~ ., data = train, family = binomial(link='logit')) 

# Predicting results in validation data 
options(warn=-1) #getting rid of checked warning messages
probabilities <- model_x %>% predict(validation, type = "response")

#predict the class of individuals/categorizes individuals into two groups based on their predicted probabilities
results <- ifelse(probabilities > 0.5,1,0)

# Actual class 
answers <- validation$Outcome_variable

# Calculation of accuracy
misClasificError <- mean(answers != results)

#Obtaining accuracy 
acc[i] <- 1-misClasificError

answers2<- as.numeric(answers) #converting to numeric so in correct format for RMSE function 

rmse <- RMSE(results,answers2) # working out root mean square error 

performances[i] <- rmse #storing RMSE value from each iteration 

# saving model(s) from each iteration

if (i == 1) {
  model_x1 <- model_x
}
if (i == 2) {
  model_x2 <- model_x
}
if (i == 3) {
  model_x3 <- model_x
}
if (i == 4) {
  model_x4 <- model_x
}
if (i == 5) {
  model_x5 <- model_x
}
if (i == 6) {
  model_x6 <- model_x
}
if (i == 7) {
  model_x7 <- model_x
}
if (i == 8) {
  model_x8 <- model_x
}
if (i == 9) {
  model_x9 <- model_x
}
if (i == 10) {
  model_x10 <- model_x
}
}


mean(acc) # average accuracy during cross-validation

print(acc) #checking accuracy across the 10 iterations

print(performances) #checking RMSE across the 10 iterations 


model_x <- model_x4 # # iteration with the lowest RMSE. Lower -> Better model

#Model information 
options(warn=-1) #getting rid of checked warning messages
summary(model_x)

# Confidence intervals 
options(warn=-1) #getting rid of checked warning messages
confint(model_x) 

# Creating odds ratio table 

options(warn=-1) #getting rid of checked warning messages
table_model <- tidy(model_x, exp=T, conf.int=T) # table with odds ratio 
View(table_model)


write.csv(table_model, file= "table_model_odds_10.csv") # saving complete odds ratio table

```


```{r}
# Obtaining additional tables based on descending odds ratio and statistical significance filtering
 
table_model2 <- table_model%>% 
arrange(desc(abs(estimate))) #arranging table by descending odds ratio value

View(table_model2)

write.csv(table_model2, file= "table_model_odds_10abs.csv") # saving table ordered on descending odds ratio value 

table_model3 <- table_model2 %>% filter(table_model2$p.value <=0.05) # filtering by statistical significance  

View(table_model3)

write.csv(table_model3, file= "table_model_odds_10abspval.csv") #saving table ordered on descending odds ratio value and filtered by statistical significance (p < 0.05)


```



```{r}
# Performance on the test data

Comfort_Q12 <- Test # saving to new variable 

# Reformatting binary variables to factors where appropriate  

Comfort_Q12$Statins<-factor(ifelse(Comfort_Q12$Statins<0.5,0,1))
Comfort_Q12$Aspirin<-factor(ifelse(Comfort_Q12$Aspirin<0.5,0,1))
Comfort_Q12$Clopidogrel<-factor(ifelse(Comfort_Q12$Clopidogrel<0.5,0,1))
Comfort_Q12$Loop.diuretics<-factor(ifelse(Comfort_Q12$Loop.diuretics<0.5,0,1))
Comfort_Q12$Anticoagulant<-factor(ifelse(Comfort_Q12$Anticoagulant<0.5,0,1))
Comfort_Q12$Replacement <- factor(ifelse(Comfort_Q12$Replacement<0.5,0,1))
Comfort_Q12$Frail <- (ifelse(Comfort_Q12$Frail<0.5,0,1))
Comfort_Q12$Charlson.Mild <- (ifelse(Comfort_Q12$Charlson.Mild<0.5,0,1))
Comfort_Q12$Charlson.Severe <- (ifelse(Comfort_Q12$Charlson.Severe<0.5,0,1))
Comfort_Q12$Upgrade <- factor(ifelse(Comfort_Q12$Upgrade<0.5,0,1))

Test <- Comfort_Q12 # reverting back to original variable name

# Making test predictions/predicting the probability of death with no appropriate therapy for each patient in the test split/set


options(warn=-1) #getting rid of checked warning messages
probabilities <- model_x %>% predict(Test, type = "response")
probabilities


# predicting the class (0 vs 1) of each patient
# Places each patient into one of the two groups based on their predicted probabilities

predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
head(predicted.classes)
table(predicted.classes)

# Assessing model accuracy

mean(predicted.classes == Test$Outcome_variable)

#Assessing RMSE

rmse_10 <- RMSE(predicted.classes,Test$Outcome_variable)

# Confusion matrix to obtain model evaluation metrics 

confusionMatrix(as.factor(predicted.classes), as.factor(Test$Outcome_variable))

```


```{r}
# Area under the precision recall curve value and precision recall curve plot 

library("PRROC")

fg <- predicted.classes[Test$Outcome_variable == 1] # scores.class0 -> all predictions for the positive class (death with no appropriate therapy)

bg <- predicted.classes[Test$Outcome_variable == 0] # scores.class1 -> all predictions for the negative class (all other outcomes)

pr <- pr.curve(scores.class0 = fg, scores.class1 = bg, curve = T) # obtaining AUPRC value. curve = T allows curve to be obtained

plot(pr, cex.main = 1) #plotting PR curve 
```


```{r}
#Area under the curve value and ROC curve plot

library(ROCR)


pred <- prediction(as.numeric(predicted.classes), as.numeric(Test$Outcome_variable)) # obtaining predictions of class vs actual class


roc = performance(pred,"tpr","fpr") #creating performance object of TPR vs FPR 

plot(roc, colorize = T, lwd = 2) #plotting ROC curve 
abline(a = 0, b = 1) 

auc = performance(pred, measure = "auc") #creating performance object of AUC

auc@y.values # obtaining AUC value
```

```{r}
# Distribution plots of all (predicted) probabilities

probabilities <- as.data.frame(probabilities) #converting to dataframe so in appropriate format for ggplot 

# plotting distribution of probabilities 

ggplot(probabilities, aes(x=probabilities)) + geom_density(aes(y = ..count..), fill = "lightgray") + geom_vline(aes(xintercept = mean(probabilities)), linetype = "dashed", size = 0.6) 
```


```{r}
# Probability distributions for (predicted) probabilities of different subsets of the test split 


Test_Outcome <- Test$Outcome_variable #storing actual outcome as integer 

Test_Outcome <- as.data.frame(Test_Outcome) # converting to dataframe 

Prob_test <-cbind(probabilities, Test_Outcome) #combining dataframes


# Probability plot for all 26 patients who died with no appropriate therapy (TP + FN)  

Prob_test_TP_FN <- filter(Prob_test, Prob_test$Test_Outcome == "1") # obtaining only TP + FN 

Prob_test_TP_FN <- Prob_test_TP_FN[-c(2)] #removing outcome variable column so only contains probabilities 

ggplot(Prob_test_TP_FN, aes(x=probabilities)) + geom_density(aes(y = ..count..), fill = "lightgray") + geom_vline(aes(xintercept = mean(probabilities)), linetype = "dashed", size = 0.6) 
mean(Prob_test_TP_FN$probabilities) # mean probability

# Probability plot for all 305 patients who didn't die with no appropriate therapy (TN + FP)

Prob_test_TN_FP <- filter(Prob_test, Prob_test$Test_Outcome == "0") # obtaining all TN + FP

Prob_test_TN_FP <- Prob_test_TN_FP[-c(2)] #removing outcome variable column so only contains probabilities 

ggplot(Prob_test_TN_FP, aes(x=probabilities)) + geom_density(aes(y = ..count..), fill = "lightgray") + geom_vline(aes(xintercept = mean(probabilities)), linetype = "dashed", size = 0.6) 

mean(Prob_test_TN_FP$probabilities)


```



